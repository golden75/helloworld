<h1> Hello World </h1>

<h3><a id="Xanadu_1"></a><span style="color: #0000ff">What is a cluster</span></h3>
A desktop or a laptop, in most cases, is inadequate for analysis of large scale datasets (e.g. Genomics) or to run simulations (e.g. protein docking).  They lack both the processing power and the memory to execute these analyses. This limitation can be overcome by combining machines (computers) in a predefined architecture/configuration so that they act as a single unit with enhanced computational power and shared resources.  This is the basic concept of a high performance cluster.  A cluster consists of a set connected computers that work together so that they can be viewed as a single system. Each computer unit in a cluster is referred as ‘node’.

<h4><a id="Xanadu_submit"></a><span style="color: #ff6600;">2. Using a submission script</span></h4>
<h4>Sample script for standard job submission.</h4>
<pre>#!/bin/bash
#SBATCH --job-name=myscript
#SBATCH -N 1
#SBATCH -n 1
#SBATCH -c 4
#SBATCH --partition=general
#SBATCH --qos=general
#SBATCH --mail-type=END
#SBATCH --mem=5G
#SBATCH --mail-user=first.last@uconn.edu
#SBATCH -o myscript_%j.out
#SBATCH -e myscript_%j.err

echo `hostname`
echo "Hello World"
module load fastqc/0.11.5
fastqc /home/CAM/$USER/raw_data/file.fastq</pre>
A general script will consist of 3 main parts:
<ul>
 	<li>The #!/bin/bash line which allows to run as a bash script</li>
 	<li>Parameters for the SLURM scheduler indicated by #SBATCH</li>
 	<li>Command submission line(s) which comes from your selected application</li>
</ul>
The #SBATCH lines indicate the set of parameters for the SLURM scheduler.
<pre><span style="color: #ff6600;">#SBATCH --job-name=myscript</span> Is the name of your script</pre>
<pre><span style="color: #ff6600;">#SBATCH -n 1</span> --ntasks Number of Task to run. The default is one task per node.</pre>
<pre><span style="color: #ff6600;">
#SBATCH -N 1</span> --nodes This line requests that the task (-n) and cores requested (-c) are all on same node. Only change this to &gt;1 if you know your code uses a message passing protocol like MPI. SLURM makes no assumptions on this parameter -- if you request more than one core (-n &gt; 1) and your forget this parameter, your job may be scheduled across nodes; and unless your job is MPI (multinode) aware, your job will run slowly, as it is oversubscribed on the master node and wasting resources on the other(s).</pre>
<pre><span style="color: #ff6600;">#SBATCH -c 4</span> --cpus-per-task number of cpus requested per task.</pre>
<pre><span style="color: #ff6600;">#SBATCH --partition=general</span> This line specifies the SLURM partition (in this instance it will be the general partition) under which the script will be run</pre>
<pre><span style="color: #ff6600;">#SBATCH --mail-type=END</span> Mailing options to indicate the state of the job. In this instance it will send a notification at the end</pre>
<pre><span style="color: #ff6600;">#SBATCH --mem=5G</span> 5Gb of memory requested (required)</pre>
<pre><span style="color: #ff6600;">#SBATCH --mail-user=first.last@uconn.edu</span> Email which the notification should be sent to</pre>
<pre><span style="color: #ff6600;">#SBATCH -o myscript_%j.out</span> Specifies the file to which the standard output will be appended, %j will add JOBID number to file name.</pre>
<pre><span style="color: #ff6600;">#SBATCH -e myscript_%j.err</span> Specifies the file to which standard error will be appended, %j will add JOBID number to file name.</pre>
<pre><span style="color: #ff6600;">echo `hostname` </span> Gets the name of the node the job is running. <span style="color: #ff0000;">We strongly recommend to put this line in every script that you are running</span>. So it will help the HPC team to determine and troubleshoot any problems when an error is encountered.</pre>
<pre><span style="color: #ff6600;">module load fastqc/0.11.5 </span> Load the software <code>fastqc</code> of version <code>0.11.5</code>. Use <code>module load</code> to load required software package with appropriate version.</pre>
<pre><span style="color: #ff6600;">fastqc /home/CAM/$USER/raw_data/file.fastq </span> Runs <code>fatstqc</code> on <code>file.fastq</code>file located at <span style="color: #ff6600;">/home/CAM/$USER/raw_data/ </span></pre>
*<strong><span style="color: #bd3115;">Always specify the absolute path to the input files. This will avoid errors and job failures in case the script is moved around.</span></strong>

*To submit a job to a particular partition <span style="color: #ff6600;">--partition</span> and <span style="color: #ff6600;">--qos</span> must be specified in the script.
To use the general partition:
<pre>--partition=general
--qos=general</pre>
To use the himem partition:
<pre>--partition=himem
--qos=himem</pre>
&nbsp;
<h4>How to submit a job</h4>
To submit a script to the cluster can be done using the <code>sbatch</code> command

<em><strong> </strong></em>All scripts are submitted to the cluster with the following command:
<pre><strong>$ <span style="color: #bd3115;">sbatch myscript.sh</span></strong></pre>
<h4>Monitoring a submitted job</h4>
To monitor all the jobs <code>squeue</code> can be used
<pre>$ <span style="color: #bd3115;">squeue</span> 
             JOBID PARTITION     NAME     USER <strong><span style="color: #99cc00;">ST</span></strong>       TIME  NODES NODELIST(REASON)
            246233   general  STO_001    USER1 <span style="color: #99cc00;">CG</span>       4:48      1 xanadu-21
            301089     himem ProtMasW    USER2 <span style="color: #99cc00;">PD</span>       0:00      1 (Priority)
            301013       amd ProtMasN    USER2  <span style="color: #99cc00;">R</span>    5:43:21      1 xanadu-24
            301677   general mv_db.sh    USER3  <span style="color: #99cc00;">R</span>      14:48      1 xanadu-22
            297400     himem  bfo_111    USER4  <span style="color: #99cc00;">R</span> 1-07:16:26      4 xanadu-[30-33]</pre>
It will give information on the jobs on all partitions. One important aspect is the state of the job in the queue.
Where;
